{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок запускать!  \n",
    "Использую python 3.10.13  \n",
    "Проверка подключения к \"https://huggingface\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подключение выполнено успешно\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def check_huggingface_connection():\n",
    "    url = \"https://huggingface.co\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Подключение выполнено успешно\")\n",
    "        else:\n",
    "            print(f\"Ошибка: статус-код {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Не удалось подключиться к Haggingface: {e}\")\n",
    "\n",
    "check_huggingface_connection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок запускать!  \n",
    "Установка модели all-MiniLM-L6-v2, эта модель используется для создания эмбеддингов (векторных представлений числа)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Загрузка модели\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Сохранение модели в папке проекта\n",
    "model.save('./models/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок запускать!    \n",
    "Проверка работы модели all-MiniLM-L6-v2 локально  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.17196208e-02  4.93621118e-02  8.15440342e-03 -8.28171708e-03\n",
      " -8.07353929e-02  1.37738073e-02  1.29308343e-01  5.55282347e-02\n",
      "  5.55311851e-02  4.64369543e-03  1.62926894e-02  2.82423906e-02\n",
      "  2.40118392e-02  1.83202177e-02 -5.01854829e-02 -5.90683632e-02\n",
      " -2.36659069e-02  4.01300676e-02  4.84813228e-02  2.36178935e-02\n",
      " -3.40621062e-02 -8.29051659e-02  7.41609707e-02  2.18984904e-03\n",
      " -6.46943450e-02  6.99526370e-02 -1.58602819e-02  5.12737818e-02\n",
      "  4.15976085e-02 -1.71494875e-02  2.88111307e-02  4.26841783e-04\n",
      "  5.18569872e-02 -2.59287544e-02  3.97441126e-02  5.15948460e-02\n",
      " -4.00950760e-02 -7.37245157e-02  5.66227967e-03  5.80665432e-02\n",
      " -4.10500132e-02 -5.81693389e-02 -7.55470395e-02  9.30383578e-02\n",
      "  4.15899754e-02  8.97833779e-02 -2.56140903e-02  3.57143134e-02\n",
      "  4.66437824e-03  2.13995781e-02 -7.20801130e-02  5.20721041e-02\n",
      " -1.11639164e-02  2.49796757e-03  4.46382985e-02 -1.05812952e-01\n",
      "  6.25862554e-02 -4.64097634e-02 -5.03810914e-03 -7.03719705e-02\n",
      " -3.48648690e-02  2.51326687e-03 -2.77373493e-02  6.27904385e-03\n",
      " -2.05784831e-02 -5.51861115e-02  2.55787000e-02  5.06049953e-02\n",
      "  3.65021359e-03  9.27590951e-02  3.87943462e-02 -2.77230144e-03\n",
      " -8.44683945e-02 -2.40003169e-02 -1.02645017e-01 -3.87629122e-02\n",
      " -2.70929672e-02 -8.31278786e-02 -5.88420480e-02  3.22500020e-02\n",
      "  4.65341583e-02 -5.53457439e-02 -1.09321937e-01 -2.24768501e-02\n",
      "  1.23491148e-02 -5.16423360e-02  2.44068708e-02 -9.25176404e-03\n",
      "  4.95268442e-02 -9.42766480e-03 -2.88384501e-02  3.42065729e-02\n",
      "  1.50072193e-02 -1.77419018e-02 -3.26235332e-02 -4.47960906e-02\n",
      " -1.25376210e-02 -7.57087246e-02 -4.30095345e-02  2.83327177e-02\n",
      " -2.82038506e-02 -1.24606937e-02  2.62527214e-03  4.31546755e-02\n",
      " -1.39866084e-01 -6.42190501e-02 -4.74442132e-02 -7.78062642e-02\n",
      "  4.05435339e-02  1.11011295e-02 -9.16558877e-02 -6.33223876e-02\n",
      " -1.44908382e-02 -2.07143240e-02  7.20818415e-02  4.07908596e-02\n",
      "  6.70586154e-02 -4.22236323e-02  1.32741593e-02 -2.36478318e-02\n",
      " -4.88307746e-03  3.31483446e-02 -4.48209088e-04  2.30483785e-02\n",
      "  2.88184881e-02  2.72227824e-03  3.44413444e-02 -1.11467841e-33\n",
      " -1.82120893e-02  6.84299134e-03 -2.80952943e-03  4.17940021e-02\n",
      " -4.46099155e-02  2.28147879e-02 -1.07335579e-02 -3.29613350e-02\n",
      "  1.21753775e-02  2.68975906e-02  3.57154873e-04 -5.22611737e-02\n",
      "  3.56578343e-02  1.02015166e-02 -8.52440600e-04  4.60845381e-02\n",
      "  4.45032818e-03  1.36803808e-02  1.11533320e-02  4.33379672e-02\n",
      "  1.57307601e-03  7.90000185e-02  9.51609015e-03  2.22887900e-02\n",
      "  5.94194718e-02 -6.64859712e-02 -1.58501379e-02 -7.99225701e-04\n",
      "  5.20409606e-02 -1.17145628e-02  9.62932315e-03 -3.89688928e-03\n",
      " -2.87405234e-02 -5.96599095e-03 -4.91821021e-02 -7.02160150e-02\n",
      " -7.41758868e-02  3.18670757e-02  3.29681672e-04  9.57412720e-02\n",
      "  5.37067391e-02 -1.20563716e-01  3.55393365e-02 -4.47949655e-02\n",
      "  6.70578182e-02  2.09845919e-02  3.10211610e-02  5.10662943e-02\n",
      "  6.93122018e-03 -4.83335443e-02 -1.55666508e-02 -1.20020043e-02\n",
      " -5.67253605e-02  3.93569022e-02 -6.31481260e-02  6.41133636e-03\n",
      "  1.21187102e-02 -5.30816568e-03 -5.28009981e-02 -5.67052066e-02\n",
      "  7.26860762e-02 -3.90625931e-02  1.87639613e-02 -4.26950604e-02\n",
      " -5.21868886e-03 -1.07134566e-01 -8.85967258e-03  6.95150122e-02\n",
      "  7.69848302e-02  1.13082357e-01 -3.57404947e-02 -3.51599343e-02\n",
      "  4.94646933e-03  3.88331972e-02  2.93501578e-02  1.92759819e-02\n",
      " -6.91940412e-02  6.64104074e-02  5.65422000e-04  2.17070226e-02\n",
      " -1.28536135e-01  6.63879067e-02  6.28814325e-02  2.84798257e-02\n",
      "  7.65739679e-02  5.29540740e-02  6.59373477e-02 -8.69807526e-02\n",
      " -3.35075781e-02  2.68258117e-02 -7.38454312e-02 -6.92359209e-02\n",
      "  9.20819268e-02  3.45078185e-02 -3.20174284e-02 -2.04937008e-33\n",
      "  2.62068901e-02 -2.85300165e-02  2.26082169e-02  2.84997318e-02\n",
      "  5.06946146e-02  2.01405808e-02 -3.22478004e-02  1.00573227e-01\n",
      " -6.01807935e-03  8.68262351e-02  7.09115341e-03 -1.00024976e-01\n",
      "  2.03004275e-02 -2.66878512e-02 -3.63310575e-02  6.46220222e-02\n",
      "  1.06245682e-01  4.04290110e-02 -1.01045787e-01  1.31966372e-03\n",
      " -7.69760460e-02 -3.58819626e-02  6.85590729e-02  1.10431593e-02\n",
      " -5.03285974e-02 -1.41847320e-02  2.16746598e-01 -4.85122763e-02\n",
      " -1.14945695e-02  3.49304900e-02 -5.91987669e-02 -1.33639090e-02\n",
      " -3.15567367e-02  4.12657578e-03  3.77259739e-02  5.73242269e-02\n",
      "  4.08281311e-02 -5.92697784e-02 -9.95462239e-02  8.46683681e-02\n",
      "  3.42919044e-02  6.37606159e-02  8.45848396e-02  8.22523236e-02\n",
      " -3.50010507e-02 -5.18739671e-02 -5.19443080e-02 -2.01748312e-02\n",
      " -7.19399154e-02 -2.64357403e-02 -5.02078328e-03 -1.48824798e-02\n",
      "  1.26748784e-02 -4.81047221e-02  3.37402970e-02 -4.42441739e-02\n",
      " -1.72602851e-02  6.23951340e-03 -1.09966630e-02  2.26278864e-02\n",
      "  5.57349878e-04 -5.47510898e-03  5.28157502e-02 -1.92809310e-02\n",
      "  1.40793331e-03 -1.32051297e-02  1.16594778e-02  8.29026923e-02\n",
      "  6.87407777e-02 -7.76162744e-03  5.70786297e-02 -4.86517362e-02\n",
      " -3.57378982e-02  6.35753199e-02 -9.88196060e-02  4.75430116e-02\n",
      " -5.90823740e-02  8.47456828e-02  9.85433608e-02 -1.51699176e-02\n",
      " -1.79467872e-02 -1.81762259e-02 -9.82780159e-02  1.96109470e-02\n",
      " -5.12683317e-02 -2.47561447e-02 -3.23161595e-02 -1.47176976e-03\n",
      "  3.85491066e-02 -3.78963687e-02  1.78951416e-02  7.96216652e-02\n",
      "  3.68369780e-02  1.26803014e-02 -1.59067791e-02 -1.91491374e-08\n",
      "  2.54985280e-02 -6.78187534e-02  1.35482429e-02  3.41795571e-02\n",
      "  4.72267717e-02 -7.95808583e-02 -8.44751969e-02 -8.96238387e-02\n",
      " -8.34285393e-02  1.90941896e-02  3.13443020e-02 -4.94542383e-02\n",
      " -5.11316620e-02 -3.47592570e-02 -1.02391178e-02  3.40193883e-02\n",
      " -4.62245494e-02  3.65735330e-02 -1.64011288e-02 -8.66905823e-02\n",
      " -3.95750180e-02 -1.84571929e-02 -3.54353860e-02 -3.07566337e-02\n",
      " -2.55409628e-02  5.54860458e-02 -5.89057151e-03 -6.76447451e-02\n",
      "  3.32299583e-02 -8.53850171e-02  8.41170549e-02  5.33572696e-02\n",
      " -7.97423795e-02 -1.71142165e-02 -1.10772029e-01  3.66585981e-03\n",
      "  4.67113927e-02  2.27213334e-02  3.68525274e-02 -3.80550921e-02\n",
      "  2.32808292e-02 -3.23126204e-02  3.40181328e-02  1.27841141e-02\n",
      " -3.07677630e-02  3.86905409e-02 -7.19043463e-02 -2.62490138e-02\n",
      " -4.46175300e-02  1.01586524e-02 -1.58395879e-02  8.55067819e-02\n",
      "  5.01329564e-02  6.53302073e-02  2.79842559e-02  9.92008820e-02\n",
      "  6.30254596e-02 -7.91557599e-03 -8.29271078e-02 -2.93763150e-02\n",
      "  7.51150399e-02  4.44857106e-02  1.62792001e-02  2.09830105e-02]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Загрузка модели из локальной папки\n",
    "model = SentenceTransformer(\"./models/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Пример использования\n",
    "embeddings = model.encode(\"Привет, мир!\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок запускать!  \n",
    "Установка модели \"open_llama_7b\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.68s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3405: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n",
      "Saving checkpoint shards: 100%|██████████| 6/6 [00:36<00:00,  6.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/open_llama_7b/tokenizer_config.json',\n",
       " './models/open_llama_7b/special_tokens_map.json',\n",
       " './models/open_llama_7b/tokenizer.model',\n",
       " './models/open_llama_7b/added_tokens.json',\n",
       " './models/open_llama_7b/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Папка для сохранения модели\n",
    "model_folder = \"./models/open_llama_7b\"\n",
    "\n",
    "# Имя модели\n",
    "model_name = \"openlm-research/open_llama_7b\"\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"  # Папка для выгрузки весов\n",
    ")\n",
    "\n",
    "# Сохранение модели локально\n",
    "model.save_pretrained(model_folder)\n",
    "tokenizer.save_pretrained(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок не запускать!  \n",
    "Проверка работы модели open_llama_7b локально.  \n",
    "Модель отработала очень медленно, ответила через 28 минут.  \n",
    "Ответ модели был:  \n",
    "\"\"\"\n",
    "Ты можешь давать ответы на вопросы?\n",
    "Да, я давал ответы на вопросы.\n",
    "Но я не могу давать ответы на вопросы.\n",
    "Я давал ответы на вопросы.\n",
    "Я давал ответы на вопросы, но я не могу давать ответы на вопросы.\n",
    "Я давал ответы на\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 14.61it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ модели:\n",
      "Ты можешь давать ответы на вопросы?\n",
      "Да, я давал ответы на вопросы.\n",
      "Но я не могу давать ответы на вопросы.\n",
      "Я давал ответы на вопросы.\n",
      "Я давал ответы на вопросы, но я не могу давать ответы на вопросы.\n",
      "Я давал ответы на\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Путь к локальной папке с моделью\n",
    "model_folder = \"./models/open_llama_7b\"\n",
    "\n",
    "# Загрузка токенизатора и модели из локальной папки\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_folder,\n",
    "    device_map=\"auto\",  # Автоматическое распределение по устройствам\n",
    "    offload_folder=\"./offload\"  # Папка для выгрузки весов (если нужно)\n",
    ")\n",
    "\n",
    "# Пример использования модели\n",
    "input_text = \"Ты можешь давать ответы на вопросы?\"\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\")  # Оставляем на CPU\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Перемещаем на GPU\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)  # Генерация ответа\n",
    "\n",
    "# Вывод результата\n",
    "print(\"Ответ модели:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок не запускать!  \n",
    "Оптимизированный код для OpenLLaMA 7B  \n",
    "Модель ответила за 22.3 сек.  \n",
    "```   \n",
    "Ответ модели:  \n",
    "как тебя зовут?\n",
    "Тексты песен и lyrics на английском и русском языках\n",
    "Тексты песен и lyrics на английском и русском языках.\n",
    "Тексты песен и lyrics на английском и русском языках. Текст\n",
    "```  \n",
    "```\n",
    "Ответ модели:\n",
    "сколько цветов в радуге?\n",
    "Добавлено: Четверг, 12.07.2018, 11:22:29 Заголовок сообщения:\n",
    "Добавлено: Среда, 18.07.2018, 11:22:29 Заголовок сообщения:\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:19<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ модели:\n",
      "сколько цветов в радуге?\n",
      "Добавлено: Четверг, 12.07.2018, 11:22:29 Заголовок сообщения:\n",
      "Добавлено: Среда, 18.07.2018, 11:22:29 Заголовок сообщения:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_folder = \"./models/open_llama_7b\"\n",
    "\n",
    "# Настройка 4-битного квантования\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_folder,\n",
    "    quantization_config=bnb_config,  # Включаем квантование\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"\n",
    ")\n",
    "\n",
    "input_text = \"сколько цветов в радуге?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(\"Ответ модели:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок запускать!\n",
    "Выполняется проверка работоспособности модели    \n",
    "Дополнительная оптимизация для OpenLLaMA 7B  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:09<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 'cuda:0'}\n",
      "VRAM used: 3.92 GB\n",
      "Ответ модели:\n",
      "Ответь на вопрос. Если на вопрос нельзя ответить, ответь 'Я не знаю'.\n",
      "Question: сколько основных цветов в радуге?\n",
      "Answer concisely:  number of main colors in the rainbow.\n",
      "Question: сколько основных цветов в радуге?\n",
      "Answer concisely: the number of main colors in the rainbow.\n",
      "Question: сколько основных цветов в радуге?\n",
      "Answer concisely: the number of main colors in the rainbow.\n",
      "Question: сколько основных цвето\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_folder = \"./models/open_llama_7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Важно для offload\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "\n",
    "device_map = {\n",
    "    \"\": \"cuda:0\",  # Все слои на GPU\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_folder,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    # offload_folder=\"./offload\"\n",
    ")\n",
    "\n",
    "print(model.hf_device_map)  # Покажет, где размещены слои\n",
    "print(f\"VRAM used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\") # Проверяем память (<6,5Gb это хорошо)\n",
    "\n",
    "input_text = \"\"\"Ответь на вопрос. Если на вопрос нельзя ответить, ответь 'Я не знаю'.\n",
    "Question: сколько основных цветов в радуге?\n",
    "Answer concisely:\"\"\"  # Чёткий формат\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(\"Ответ модели:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок запускать!  \n",
    "Использую python 3.10.13\n",
    "1. Проверяем, что PyTorch использует CUDA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch видит CUDA: True\n",
      "Количество GPU: 1\n",
      "Текущий GPU: 0\n",
      "Название GPU: NVIDIA T1000 8GB\n",
      "Версия CUDA в PyTorch: 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch видит CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"Количество GPU: {torch.cuda.device_count()}\")\n",
    "print(f\"Текущий GPU: {torch.cuda.current_device()}\")\n",
    "print(f\"Название GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Версия CUDA в PyTorch: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок запускать!  \n",
    "2. Проверяем загрузку модели на GPU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.12it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена на: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"./models/open_llama_7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "print(f\"Модель загружена на: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "конфигурация для T1000 8GB  \n",
    "1. 4-битное квантование \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступно VRAM: 0.00 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:10<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сколько цветов в радуге? Ответь числом:\n",
      "Рассчитайте это с помощью\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_path = \"./models/open_llama_7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True  # Дополнительное сжатие\n",
    ")\n",
    "\n",
    "print(f\"Доступно VRAM: {torch.cuda.memory_reserved(0)/1024**3:.2f} GiB\")\n",
    "\n",
    "torch.cuda.empty_cache()  # Очистка кэша\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # use_flash_attention_2=True  # Ускорение\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "inputs = tokenizer(\"Сколько цветов в радуге? Ответь числом:\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,  # Короткие ответы\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_k=40\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура системы  \n",
    "    A[Запрос пользователя] --> B[Создание эмбеддинга (all-MiniLM-L6-v2)]  \n",
    "    B --> C[Поиск в PostgreSQL]  \n",
    "    C --> D[Формирование контекста]  \n",
    "    D --> E[Генерация ответа (open_llama-3b)]  \n",
    "    E --> F[Ответ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок запускать!   \n",
    "Скачивание модели open_llama_3b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.10/site-packages (0.29.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files: 100%|██████████| 8/8 [02:23<00:00, 17.88s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/parfenov/projects/chat-bot_for_site_whis_llm/models/open_llama_3b'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Скачивание модели в папку .models/\n",
    "snapshot_download(\n",
    "    repo_id=\"openlm-research/open_llama_3b\",\n",
    "    local_dir=\"./models/open_llama_3b\",\n",
    "    local_dir_use_symlinks=False,\n",
    "    resume_download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок запускать!  \n",
    "Проверка моедли open_llama_3b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сколько основных цветов в радуге? Ответь только числом: 10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"./models/open_llama_3b\"\n",
    "\n",
    "# 1. Загружаем токенизатор - он преобразует текст в числа\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 2. Загружаем саму модель для генерации текста\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",  # Автоматически использует GPU\n",
    "    torch_dtype=torch.float16  # Используем меньше памяти\n",
    ")\n",
    "\n",
    "# 3. вопрос для модели\n",
    "input_text = \"Сколько основных цветов в радуге? Ответь только числом:\"\n",
    "\n",
    "# 4. Преобразуем текст в токены\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 5. Генерируем ответ\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=5,  # длина ответа\n",
    "    temperature=0.1  # Делаем ответ более точным\n",
    ")\n",
    "\n",
    "# 6. Преобразуем числа обратно в текст и выводим\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для тестирования модели через LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_129578/645446002.py:26: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Сколько основных цветов в радуге? Ответь числом.\n",
      "Например, 1000 ц\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# 1. Указываем путь к модели\n",
    "model_path = \"./models/open_llama_3b\"\n",
    "\n",
    "# 2. Загружаем модель и токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 3. Создаем конвейер для генерации текста\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=15,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 4. Обертка для LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 5. Задаем вопрос и получаем ответ\n",
    "question = \"Сколько основных цветов в радуге? Ответь числом.\"\n",
    "response = llm.invoke(question)\n",
    "\n",
    "print(\"Ответ:\", response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорение генерации  \n",
    "Повышение точности ответов. Фикс шаблона промпта  \n",
    "Оптимальные параметры генерации  \n",
    "Подготовка к RAG  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_131148/439096708.py:31: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Сколько граней у куба? Ответь числом: 10.\n",
      "Ответ: 10. Так как 10 граней у куба, то 10 граней у куба.\n",
      "Ответ: 10. Так как 10 граней у куба, то 10 граней у куба.\n",
      "Ответ: 10\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Конфиг для 4-битного квантования\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # сжимаем модель в 4 раза\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Загрузка модели\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./models/open_llama_3b\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/open_llama_3b\")\n",
    "\n",
    "# Конвейер генерации\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.01,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# LangChain интерфейс\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "response = llm.invoke(\"Вопрос: Сколько граней у куба? Ответь числом:\")\n",
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переходим на модель Phi-2  \n",
    "скачиваем моель Phi-2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Скачивание модели в папку .models/\n",
    "snapshot_download(\n",
    "    repo_id=\"openlm-research/open_llama_3b\",\n",
    "    local_dir=\"./models/open_llama_3b\",\n",
    "    local_dir_use_symlinks=False,\n",
    "    resume_download=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
