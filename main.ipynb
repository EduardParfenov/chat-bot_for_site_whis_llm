{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "использую python 3.11.1  \n",
    "Проверка подключения к \"https://huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подключение выполнено успешно\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def check_huggingface_connection():\n",
    "    url = \"https://huggingface.co\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Подключение выполнено успешно\")\n",
    "        else:\n",
    "            print(f\"Ошибка: статус-код {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Не удалось подключиться к Haggingface: {e}\")\n",
    "\n",
    "check_huggingface_connection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка модели all-MiniLM-L6-v2, эта модель используется для создания эмбеддингов (векторных представлений числа)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Загрузка модели\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Сохранение модели в папке проекта\n",
    "model.save('./models/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка работы модели all-MiniLM-L6-v2 локально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.17196208e-02  4.93621118e-02  8.15440342e-03 -8.28171708e-03\n",
      " -8.07353929e-02  1.37738073e-02  1.29308343e-01  5.55282347e-02\n",
      "  5.55311851e-02  4.64369543e-03  1.62926894e-02  2.82423906e-02\n",
      "  2.40118392e-02  1.83202177e-02 -5.01854829e-02 -5.90683632e-02\n",
      " -2.36659069e-02  4.01300676e-02  4.84813228e-02  2.36178935e-02\n",
      " -3.40621062e-02 -8.29051659e-02  7.41609707e-02  2.18984904e-03\n",
      " -6.46943450e-02  6.99526370e-02 -1.58602819e-02  5.12737818e-02\n",
      "  4.15976085e-02 -1.71494875e-02  2.88111307e-02  4.26841783e-04\n",
      "  5.18569872e-02 -2.59287544e-02  3.97441126e-02  5.15948460e-02\n",
      " -4.00950760e-02 -7.37245157e-02  5.66227967e-03  5.80665432e-02\n",
      " -4.10500132e-02 -5.81693389e-02 -7.55470395e-02  9.30383578e-02\n",
      "  4.15899754e-02  8.97833779e-02 -2.56140903e-02  3.57143134e-02\n",
      "  4.66437824e-03  2.13995781e-02 -7.20801130e-02  5.20721041e-02\n",
      " -1.11639164e-02  2.49796757e-03  4.46382985e-02 -1.05812952e-01\n",
      "  6.25862554e-02 -4.64097634e-02 -5.03810914e-03 -7.03719705e-02\n",
      " -3.48648690e-02  2.51326687e-03 -2.77373493e-02  6.27904385e-03\n",
      " -2.05784831e-02 -5.51861115e-02  2.55787000e-02  5.06049953e-02\n",
      "  3.65021359e-03  9.27590951e-02  3.87943462e-02 -2.77230144e-03\n",
      " -8.44683945e-02 -2.40003169e-02 -1.02645017e-01 -3.87629122e-02\n",
      " -2.70929672e-02 -8.31278786e-02 -5.88420480e-02  3.22500020e-02\n",
      "  4.65341583e-02 -5.53457439e-02 -1.09321937e-01 -2.24768501e-02\n",
      "  1.23491148e-02 -5.16423360e-02  2.44068708e-02 -9.25176404e-03\n",
      "  4.95268442e-02 -9.42766480e-03 -2.88384501e-02  3.42065729e-02\n",
      "  1.50072193e-02 -1.77419018e-02 -3.26235332e-02 -4.47960906e-02\n",
      " -1.25376210e-02 -7.57087246e-02 -4.30095345e-02  2.83327177e-02\n",
      " -2.82038506e-02 -1.24606937e-02  2.62527214e-03  4.31546755e-02\n",
      " -1.39866084e-01 -6.42190501e-02 -4.74442132e-02 -7.78062642e-02\n",
      "  4.05435339e-02  1.11011295e-02 -9.16558877e-02 -6.33223876e-02\n",
      " -1.44908382e-02 -2.07143240e-02  7.20818415e-02  4.07908596e-02\n",
      "  6.70586154e-02 -4.22236323e-02  1.32741593e-02 -2.36478318e-02\n",
      " -4.88307746e-03  3.31483446e-02 -4.48209088e-04  2.30483785e-02\n",
      "  2.88184881e-02  2.72227824e-03  3.44413444e-02 -1.11467841e-33\n",
      " -1.82120893e-02  6.84299134e-03 -2.80952943e-03  4.17940021e-02\n",
      " -4.46099155e-02  2.28147879e-02 -1.07335579e-02 -3.29613350e-02\n",
      "  1.21753775e-02  2.68975906e-02  3.57154873e-04 -5.22611737e-02\n",
      "  3.56578343e-02  1.02015166e-02 -8.52440600e-04  4.60845381e-02\n",
      "  4.45032818e-03  1.36803808e-02  1.11533320e-02  4.33379672e-02\n",
      "  1.57307601e-03  7.90000185e-02  9.51609015e-03  2.22887900e-02\n",
      "  5.94194718e-02 -6.64859712e-02 -1.58501379e-02 -7.99225701e-04\n",
      "  5.20409606e-02 -1.17145628e-02  9.62932315e-03 -3.89688928e-03\n",
      " -2.87405234e-02 -5.96599095e-03 -4.91821021e-02 -7.02160150e-02\n",
      " -7.41758868e-02  3.18670757e-02  3.29681672e-04  9.57412720e-02\n",
      "  5.37067391e-02 -1.20563716e-01  3.55393365e-02 -4.47949655e-02\n",
      "  6.70578182e-02  2.09845919e-02  3.10211610e-02  5.10662943e-02\n",
      "  6.93122018e-03 -4.83335443e-02 -1.55666508e-02 -1.20020043e-02\n",
      " -5.67253605e-02  3.93569022e-02 -6.31481260e-02  6.41133636e-03\n",
      "  1.21187102e-02 -5.30816568e-03 -5.28009981e-02 -5.67052066e-02\n",
      "  7.26860762e-02 -3.90625931e-02  1.87639613e-02 -4.26950604e-02\n",
      " -5.21868886e-03 -1.07134566e-01 -8.85967258e-03  6.95150122e-02\n",
      "  7.69848302e-02  1.13082357e-01 -3.57404947e-02 -3.51599343e-02\n",
      "  4.94646933e-03  3.88331972e-02  2.93501578e-02  1.92759819e-02\n",
      " -6.91940412e-02  6.64104074e-02  5.65422000e-04  2.17070226e-02\n",
      " -1.28536135e-01  6.63879067e-02  6.28814325e-02  2.84798257e-02\n",
      "  7.65739679e-02  5.29540740e-02  6.59373477e-02 -8.69807526e-02\n",
      " -3.35075781e-02  2.68258117e-02 -7.38454312e-02 -6.92359209e-02\n",
      "  9.20819268e-02  3.45078185e-02 -3.20174284e-02 -2.04937008e-33\n",
      "  2.62068901e-02 -2.85300165e-02  2.26082169e-02  2.84997318e-02\n",
      "  5.06946146e-02  2.01405808e-02 -3.22478004e-02  1.00573227e-01\n",
      " -6.01807935e-03  8.68262351e-02  7.09115341e-03 -1.00024976e-01\n",
      "  2.03004275e-02 -2.66878512e-02 -3.63310575e-02  6.46220222e-02\n",
      "  1.06245682e-01  4.04290110e-02 -1.01045787e-01  1.31966372e-03\n",
      " -7.69760460e-02 -3.58819626e-02  6.85590729e-02  1.10431593e-02\n",
      " -5.03285974e-02 -1.41847320e-02  2.16746598e-01 -4.85122763e-02\n",
      " -1.14945695e-02  3.49304900e-02 -5.91987669e-02 -1.33639090e-02\n",
      " -3.15567367e-02  4.12657578e-03  3.77259739e-02  5.73242269e-02\n",
      "  4.08281311e-02 -5.92697784e-02 -9.95462239e-02  8.46683681e-02\n",
      "  3.42919044e-02  6.37606159e-02  8.45848396e-02  8.22523236e-02\n",
      " -3.50010507e-02 -5.18739671e-02 -5.19443080e-02 -2.01748312e-02\n",
      " -7.19399154e-02 -2.64357403e-02 -5.02078328e-03 -1.48824798e-02\n",
      "  1.26748784e-02 -4.81047221e-02  3.37402970e-02 -4.42441739e-02\n",
      " -1.72602851e-02  6.23951340e-03 -1.09966630e-02  2.26278864e-02\n",
      "  5.57349878e-04 -5.47510898e-03  5.28157502e-02 -1.92809310e-02\n",
      "  1.40793331e-03 -1.32051297e-02  1.16594778e-02  8.29026923e-02\n",
      "  6.87407777e-02 -7.76162744e-03  5.70786297e-02 -4.86517362e-02\n",
      " -3.57378982e-02  6.35753199e-02 -9.88196060e-02  4.75430116e-02\n",
      " -5.90823740e-02  8.47456828e-02  9.85433608e-02 -1.51699176e-02\n",
      " -1.79467872e-02 -1.81762259e-02 -9.82780159e-02  1.96109470e-02\n",
      " -5.12683317e-02 -2.47561447e-02 -3.23161595e-02 -1.47176976e-03\n",
      "  3.85491066e-02 -3.78963687e-02  1.78951416e-02  7.96216652e-02\n",
      "  3.68369780e-02  1.26803014e-02 -1.59067791e-02 -1.91491374e-08\n",
      "  2.54985280e-02 -6.78187534e-02  1.35482429e-02  3.41795571e-02\n",
      "  4.72267717e-02 -7.95808583e-02 -8.44751969e-02 -8.96238387e-02\n",
      " -8.34285393e-02  1.90941896e-02  3.13443020e-02 -4.94542383e-02\n",
      " -5.11316620e-02 -3.47592570e-02 -1.02391178e-02  3.40193883e-02\n",
      " -4.62245494e-02  3.65735330e-02 -1.64011288e-02 -8.66905823e-02\n",
      " -3.95750180e-02 -1.84571929e-02 -3.54353860e-02 -3.07566337e-02\n",
      " -2.55409628e-02  5.54860458e-02 -5.89057151e-03 -6.76447451e-02\n",
      "  3.32299583e-02 -8.53850171e-02  8.41170549e-02  5.33572696e-02\n",
      " -7.97423795e-02 -1.71142165e-02 -1.10772029e-01  3.66585981e-03\n",
      "  4.67113927e-02  2.27213334e-02  3.68525274e-02 -3.80550921e-02\n",
      "  2.32808292e-02 -3.23126204e-02  3.40181328e-02  1.27841141e-02\n",
      " -3.07677630e-02  3.86905409e-02 -7.19043463e-02 -2.62490138e-02\n",
      " -4.46175300e-02  1.01586524e-02 -1.58395879e-02  8.55067819e-02\n",
      "  5.01329564e-02  6.53302073e-02  2.79842559e-02  9.92008820e-02\n",
      "  6.30254596e-02 -7.91557599e-03 -8.29271078e-02 -2.93763150e-02\n",
      "  7.51150399e-02  4.44857106e-02  1.62792001e-02  2.09830105e-02]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Загрузка модели из локальной папки\n",
    "model = SentenceTransformer(\"./models/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Пример использования\n",
    "embeddings = model.encode(\"Привет, мир!\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка модели \"open_llama_7b\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Downloading shards: 100%|██████████| 2/2 [05:41<00:00, 170.73s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.98s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:2862: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n",
      "Saving checkpoint shards: 100%|██████████| 6/6 [00:58<00:00,  9.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/open_llama_7b/tokenizer_config.json',\n",
       " './models/open_llama_7b/special_tokens_map.json',\n",
       " './models/open_llama_7b/tokenizer.model',\n",
       " './models/open_llama_7b/added_tokens.json',\n",
       " './models/open_llama_7b/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Папка для сохранения модели\n",
    "model_folder = \"./models/open_llama_7b\"\n",
    "\n",
    "# Имя модели\n",
    "model_name = \"openlm-research/open_llama_7b\"\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"  # Папка для выгрузки весов\n",
    ")\n",
    "\n",
    "# Сохранение модели локально\n",
    "model.save_pretrained(model_folder)\n",
    "tokenizer.save_pretrained(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка работы модели open_llama_7b локально.  \n",
    "Модель отработала очень медленно, ответила через 28 минут.  \n",
    "Ответ модели был:  \n",
    "\"\"\"\n",
    "Ты можешь давать ответы на вопросы?\n",
    "Да, я давал ответы на вопросы.\n",
    "Но я не могу давать ответы на вопросы.\n",
    "Я давал ответы на вопросы.\n",
    "Я давал ответы на вопросы, но я не могу давать ответы на вопросы.\n",
    "Я давал ответы на\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.23it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ модели:\n",
      "Ты можешь давать ответы на вопросы?\n",
      "Да, я давал ответы на вопросы.\n",
      "Но я не могу давать ответы на вопросы.\n",
      "Я давал ответы на вопросы.\n",
      "Я давал ответы на вопросы, но я не могу давать ответы на вопросы.\n",
      "Я давал ответы на\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Путь к локальной папке с моделью\n",
    "model_folder = \"./models/open_llama_7b\"\n",
    "\n",
    "# Загрузка токенизатора и модели из локальной папки\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_folder,\n",
    "    device_map=\"auto\",  # Автоматическое распределение по устройствам\n",
    "    offload_folder=\"./offload\"  # Папка для выгрузки весов (если нужно)\n",
    ")\n",
    "\n",
    "# Пример использования модели\n",
    "input_text = \"Ты можешь давать ответы на вопросы?\"\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\")  # Оставляем на CPU\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Перемещаем на GPU\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)  # Генерация ответа\n",
    "\n",
    "# Вывод результата\n",
    "print(\"Ответ модели:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизированный код для OpenLLaMA 7B  \n",
    "Модель ответила за 22.3 сек.  \n",
    "Ответ модели:  \n",
    "\"\"\"как тебя зовут?\n",
    "Тексты песен и lyrics на английском и русском языках\n",
    "Тексты песен и lyrics на английском и русском языках.\n",
    "Тексты песен и lyrics на английском и русском языках. Текст\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ модели:\n",
      "сколько цветов в радуге?\n",
      "Добавлено: Четверг, 12.07.2018, 11:22:29 Заголовок сообщения:\n",
      "Добавлено: Среда, 18.07.2018, 11:22:29 Заголовок сообщения:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_folder = \"./models/open_llama_7b\"\n",
    "\n",
    "# Настройка 4-битного квантования\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_folder,\n",
    "    quantization_config=bnb_config,  # Включаем квантование\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"\n",
    ")\n",
    "\n",
    "input_text = \"сколько цветов в радуге?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(\"Ответ модели:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительная оптимизация для OpenLLaMA 7B  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parfenov/projects/chat-bot_for_site_whis_llm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 'cuda:0'}\n",
      "VRAM used: 3.93 GB\n",
      "Ответ модели:\n",
      "Question: сколько основных цветов в радуге?\n",
      "Answer concisely: Основные цвета радуги: Белый, Зелёный, Розовый, Чёрный.\n",
      "Question: сколько основных цветов в радуге?\n",
      "Answer concisely: Основные цвета радуги: Белый, Зе\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_folder = \"./models/open_llama_7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Важно для offload\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "\n",
    "device_map = {\n",
    "    \"\": \"cuda:0\",  # Все слои на GPU\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_folder,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    # offload_folder=\"./offload\"\n",
    ")\n",
    "\n",
    "print(model.hf_device_map)  # Покажет, где размещены слои\n",
    "print(f\"VRAM used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\") # Проверяем память (<6,5Gb это хорошо)\n",
    "\n",
    "input_text = \"\"\"Question: сколько основных цветов в радуге?\n",
    "Answer concisely:\"\"\"  # Чёткий формат\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(\"Ответ модели:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
